# 目录



## 一、概述

### **摘要**

​	随着个人电脑使用者的增多和互联网技术的普及与发展，企业进行招聘的方式也发生了很大的改变。从早期主要是内部推荐、张贴海报，到较早期发展为在报纸、杂志、电视或广播电台上发布招聘广告，再到到外地举办大型招聘会等方式，直至现如今网络招聘的兴起，可以看出，企业愈来愈主动拓展眼界，积极向外寻找适合的人才。与此相协调的是，而今求职者在寻找合适职位时，不仅仅局限于所在地区的企业，对于网络招聘的依赖程度日益增高。网络有效地拉近了全国范围内企业与求职者的距离。招聘的网络化已成为一种普遍的招聘模式。

​	然而，在线招聘并非一片净土，与之相反，由于网络平台的开发性、不安全性以及相关制度和法律的不健全等等原因，在线招聘平台逐渐变成了欺骗者的“无法无天之地”。更加令人沮丧的是，如今的虚假招聘变得越来越难以与真实招聘区分开来，越来越多的求职者陷入发布虚假招聘者的圈套之中。

### **项目目标**

1. 具体的研究在线招聘欺诈的各个方面
2. 提供我们使用的在线招聘数据集
3. 详细的分析数据集，并根据数据集给出判定招聘是否虚假的参考规则集
4. 提出给予机器学习的在线招聘欺诈检测的解决方案
5. 提供检测的使用接口及扩展接口
6. 如果时间及精力允许，实现一个 demo

### **项目原则**

1. 客观性

   保证项目从始至终进行的过程客观的看待每一个步骤

2. 真实性

   保证项目用到的所有数据及其他信息均真实且来源可靠

3. 有效性

   保证所有项目理论及项目成果测试有效，争取满足商业使用

4. 易用性

   保证项目提供的数据集、源程序或者成果易于使用，项目代码尽量保证模块化、自动化，项目结构保证解耦规范

5. 可扩展性

   保证项目成果（特别是模型训练结果及输入数据集）具有高度的可扩展性

## 二、项目总体分析

- **业务分析**

  | 内容           | 分析设计                                                     |
  | -------------- | ------------------------------------------------------------ |
  | 项目名称       | 基于机器学习的在线招聘欺诈检测                               |
  | 项目背景及价值 | 大多数情况下，**在线招聘欺诈**往往会带来以下危害（包括但不限于）：<br/>**对于求职者：**<br/>    1. 泄露求职者的简历上的一切隐私信息<br/>    2. 求职者可能陷入金融诈骗，造成经济损失<br/>    3. 求职者可能进入一些“表里不一”的企业，造成经济、精神损失<br/>    4. 求职者可能被骗入培训机构甚至传销组织，危害人身安全。<br/>**对于公司或者组织：**<br/>    1. 被欺骗者冒用的公司会受到信誉及经济的损失<br/>**针对在线虚假招聘及虚假招聘信息，我们给出的定义为：**<br/>    **在线招聘欺诈**是一种恶意行为，是指通过操纵并利用ATS系统的功能，并发布虚假招聘信息的行为。<br/>    **虚假招聘信息**是指任何不以招聘人才为目的或招聘内容具有煽动性且缺乏一定真实性的招聘信息。<br/> |
  | 业务需求       | 利用 58 同城的在线招聘信息，通过机器学习，从这些数据中构建模型，用于检测测试在线招聘信息的虚假程度 |
  | 非功能性需求   | 检测虚假招聘的成功率应达到 80% 以上。                        |

- **系统分析**

  | 内容                       | 分析设计                                                     |
  | -------------------------- | ------------------------------------------------------------ |
  | 基本训练模型方案           | 1. 由于有明确的检测结果要求，因此将使用有监督的模型；<br/>2. 由于检测结果是具体数值（可能为招聘虚假的概率），而非标签，因此属于回归问题 |
  | 模型效果的测量指标         | 回归分析模型的性能，可采用的测量指标有：                     |
  | 开发环境                   | Python 3.7 + Scikit-Learn 为主，Numpy + Pandas 为辅。        |
  | 易用性、可重用性及可扩展性 | 毋庸置疑                                                     |
  | 前提假设                   |                                                              |
  | 术语说明                   | 主要用于说明程序中有关数据集的变量名称<br /><table><tr><td>变量名</td><td>变量含义</td></tr></table><br />    略 |

## 二、原始数据分析

- **数据源分析**

  | 内容                             | 分析设计                                                     |
  | -------------------------------- | ------------------------------------------------------------ |
  | 数据来源                         | 使用的数据为 58 同城平台上公司发布的招聘信息                 |
  | 数据内容及数据量                 | 相比于智联招聘的平台，58 同城上面的在线招聘欺诈情况更为严峻，所获取的数据更有利于进行针对性的研究<br />数据量不宜过少，但过多也不现实，最终的数据总量为 2w 余条招聘信息，大小为 60 余 MB |
  | 数据授权情况、访问权限及法律义务 | 项目用到的数据，属于平台对外公开的招聘信息，可以合理使用与项目中，但不确定是否需要授权及承担法律责任 |

- **获取数据分析**

  | 内容                           | 分析设计                                                     |
  | ------------------------------ | ------------------------------------------------------------ |
  | 获取数据                       | 1. 数据源：[58 同城](https://58.com)<br />2. 获取数据：[58 同城招聘信息爬虫](<https://github.com/ORFD/spider>) |
  | 人工标记数据                   | 对爬取的数据进行人工判别标注，判别规则手册见：<br />[手工判别招聘信息规则说明](./schedules/manual.md) |
  | 抽取原始测试数据集及过程自动化 | 需要从原始数据集中抽取一部分原始且未经任何处理的数据或者从数据来源获取一部分数据，用于验证模型的实际性能<br />该原始数据集不同于交叉验证时的数据集，它是未经任何处理的数据<br />以上过程都需自动化完成。 |

## 三、 数据特征分析

该步骤主要是对所获取的数据进行详细分析，可以使我们清楚地了解需要处理的数据集的基本特征和业务特征，为下一步的特征工程提供支持。该步骤不修改信息，也没有相应的自动化处理代码，需要记录分析结果

| 内容                                   | 分析设计                                                     |
| -------------------------------------- | ------------------------------------------------------------ |
| 装载数据集到内存并创建数据集的一个副本 | 略                                                           |
| 检查数据集大小                         | 略                                                           |
| 获取数据集的所有特征名称及数据类型     | 略                                                           |
| 确定数据集的标签列                     | **Real/Fake**                                                |
| 统计数据集各个特征的分布情况           | 略                                                           |
| 分析特征属性的基本含义                 | 数据详细介绍请参考[58 同城数据库说明及分析](./DB.md)         |
| 数据集的快速统计分析                   | 略                                                           |
| 可视化数据进行分析                     | 1. 每个特征列的直方图<br />2. 散点图<br />3. 等等等等        |
| 分析样本属性中的相关性                 | 特征属性与标签列及特征属性之间的相关性<br />1. 线性相关<br />2. 数学表达式相关<br />3. 非线性相关 |
| 属性分割、属性结合及属性剔除           | 1. 根据前一过程得到的特征属性间的相关性选择需要分割和结合的属性<br />2. 根据特征属性与标签列间的相关性选择需要剔除的属性：如相关性很弱的属性就需要剔除 |
| 完整详细的记录上述内容                 | 包括图片、文档等资源                                         |

## 四、特征工程

机器学习最关键的就是特征工程。该步骤将原始数据集经过：**特征预处理 -> 特征构建 -> 特征提取 -> 特征选择**，得到描述/代表一条数据的最优特征向量，**构建出最终的输入数据集**。

| 内容                                               | 分析设计                                                     |
| -------------------------------------------------- | ------------------------------------------------------------ |
| 创建一个特征数据集的副本                           | 该步骤处理的数据集不包括标签列                               |
| 根据变量缺失类型填充或剔除异常值/缺失值/离群值     | 1. 剔除：变量缺失度很高且变量对输出结果影响很小，则可以直接剔除（即删除列）<br />2. 填充：根据缺失变量类型填充<br />**连续变量**<br /><br />> 将数据集中不含缺失值的变量（属性）称为完全变量，数据集中含有缺失值的变量称为不完全变量，Little 和 Rubin定义了以下三种不同的数据缺失机制：<br/>    2.1 完全随机缺失（Missing Completely at Random，MCAR）。数据的缺失与不完全变量以及完全变量都是无关的。一般为填充均值或者从其他非缺失值中抽样填充<br />    2.2 随机缺失（Missing at Random，MAR）。数据的缺失仅仅依赖于完全变量。在同一行内，根据依赖的变量进行填充<br/>    2.3 非随机、不可忽略缺失（Not Missing at Random, NMAR，or nonignorable）。不完全变量中数据的缺失依赖于不完全变量本身，这种缺失是不可忽略的。可以直接对缺失与否进行二值化或者多值化转换为类别变量<br />**类别变量**<br />    2.4 同完全随机缺失<br />**对于异常值/离群值可同视为缺失值看待** |
| 通过属性剔除、属性结合、属性替代、属性分割去相关性 | **属性的可解释性可理解为属性是否易于理解和说明，解释性越高越好**<br />**属性剔除**<br />1. 对输出结果无影响或者影响过小的剔除<br />**属性结合**<br />2. 两属性之间相关性过大，可以将两属性转换为一个属性，且保留解释性较强的那一个<br />**属性替代**<br />3. 对于具有如下相关性特点的属性，可以进行属性替代，将解释性弱的属性替代为解释性强的属性<br />    3.1 数学公式相关<br />    3.2 线性相关<br />如对于**简历申请数目**与**简历阅读百分比**，则可以将简历阅读百分比替代为**简历阅读数目**<br />**属性分割**<br />4. 对于两个或多个属性间相关性较强且无法进行属性结合的属性可以对其进行分割，分割为多个解释性强的属性。 |
| 对类别型属性进行编码                               | 如二值化处理、设置 step、**dummy**编码、**one-hot**编码等等  |
| 对连续性数据离散化处理                             | 常用的途径有：等距划分、等频划分、**卡方检验**等等           |
| 特征归一化、标准化、正则化                         | 总的来说，归一化是为了消除不同数据之间的量纲，方便数据比较和共同处理，比如在神经网络中，归一化可以加快训练网络的收敛性；标准化是为了方便数据的下一步处理，而进行的数据缩放等变换，并不是为了方便与其他数据一同处理或比较，比如数据经过零-均值标准化后，更利于使用标准正态分布的性质，进行处理；正则化而是利用先验知识，在处理过程中引入正则化因子(regulator)，增加引导约束的作用，比如在逻辑回归中使用正则化，可有效降低过拟合的现象。<br />**目前不去深究** |
| 文本属性处理                                       | **很关键**                                                   |
| 特征提取                                           | 上述过程为特征预处理及特征构建的过程<br />特征提取是对上述特征构建的结果特征向量进行特征转换从而达到降维以及进一步去相关的目的的过程，特征提取有时能发现更有意义的特征属性。主要方法为主成分分析、LDA等等 |
| 特征选择                                           | 与特征提取不同的是，特征选择不进行特征转换，而是直接从特征中筛除一部分冗余或者对输出不相干的特征的过程，严格点说：特征选择是从特征集合之中挑选一组最具代表意义的特征子集，从而达到降维效果的过程<br />**特征选择主要有以下两个功能**<br />1. 减少特征数量，降维，使模型泛化能力更强<br />2. 减少过拟合，增强模型对特征与特征值之间的理解<br />特征选择一般分为以下两个步骤：<br />1. 特征子集生成/选择：常用的特征选择方法分为过滤法(Filter)、包装法(Wrapper)和集成法(Embedded)<br />    1.1 **过滤法**：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征<br />    1.2 **包装法**：根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。有逐步回归（Stepwise regression）、向前选择（Forward selection）和向后选择（Backward selection）<br />    1.3 **集成法**：集成法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。Regularization，或者使用决策树思想，Random Forest和Gradient boosting等<br />**具体描述略过**<br />2. 特征子集验证：主要为对特征集合进行有效性分析，得到每个属性对于输出结果影响的权重，可分为：<br />    2.1 与模型相关特征权重<br />    2.2 与模型无关特征权重 |
| 自动化处理过程                                     | 易用性、可扩展性                                             |
## 五、训练模型

该步骤将使用特征数据集对多种模型进行训练及优化，并对训练结果进行评估对比选出最优模型，最后使用最优模型验证原始测试数据集，分析测试结果。

| 内容                                                         | 分析设计                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 从数据集中抽取一定数目数据进行快速测试                       | 略                                                           |
| 先对所有模型分别进行训练                                     | 保证代码的可扩展性和易用性：选择模型->传入训练数据集->训练模型，采用交叉验证策略，并保存每次测试训练得到的模型，以便后续的分析。 |
| 使用标准参数对不同的模型进行训练对比评估性能                 | 模型包括：<br />- 随机森林<br />- 朴素贝叶斯<br />- 决策树   |
| 调整模型参数，优化模型                                       | 需要分别对多个模型的参数进行调整，可以使用系统自带的优化器   |
| 利用多进程或多线程同时训练多个模型，并对比评估最终的模型性能 | 此步骤可以略过                                               |
| 上述训练过程自动化                                           | **可扩展性**，提供模型使用及训练的接口                       |

## 六、对原始测试数据集进行测试

| 内容               | 分析设计                                                     |
| ------------------ | ------------------------------------------------------------ |
| 测试原始测试数据集 | 原始测试数据集应经过上述所有数据处理过程，然后选取模型训练阶段获得的最佳模型对输入测试数据集进行验证测试 |
| 分析测试结果       | 对检测结果正确率、属性特征权重等等信息进行可视化数学分析等步骤。 |

## 七、后续工作

| 内容                                     | 分析设计                                          |
| ---------------------------------------- | ------------------------------------------------- |
| 将上述所有过程以及之前的数据采集形成接口 | 略                                                |
| 将项目的数据集构建为公告数据集的形式     | 略                                                |
| 根据接口实现一个在线招聘欺诈检测的 demo  | 网站门户的形式                                    |
| 解决上述过程残留的问题                   | 如数据授权问题等等                                |
| 生成项目结果分析报告，总结项目所有文档   | 包括项目经验总结，项目论文，项目说明等等          |
| 作品提交                                 | 提交包括文档及作品 demo，不包括源码及所有数据集。 |

## 八、进度安排（暂无）

| 时间    | 工作                                         |
| ------- | -------------------------------------------- |
| 5.6-5.7 | 数据人工识别及讨论核对完毕，开始进行数据汇总 |
| 略      | 略                                           |

## 九、人员分工（暂无）

以下为分工格式

| 人员姓名 | 工作 | 工作时间 | 完成情况 |
| -------- | ---- | -------- | -----|
| z  | 略   | 5.6-5.7  | 50% |
| l    | 略   | 5.6-5.8  | 0% |
| z  | 略   | 5.8-5.10 | 80% |

