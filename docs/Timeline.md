## 时间历程
我想先总结一下，从项目正式开始到现在，我都做了些什么。
### 3.14-3.20
> 智联招聘爬虫 1.0 完成

3.14：智联招聘接口获取解析完成
3.15-3.17：初步获取智联招聘信息，并根据智联接口建立了company及recruitment 两个信息表
3.17-3.18：实现了大规模爬虫所需的代理池，根据开源项目修改，修正部分爬虫代码
3.19：实现了大规模爬虫所需的线程池，修正代码
3.20：所有子模块，包括代理池、线程池、数据库操作模块、爬虫核心模块、主控制调度模块、及配置模块组合在一起，智联招聘爬虫1.0 完成

### 3.21-3.31
> 爬取智联招聘俩万余条数据

其间多次修正代码bug，优化数据库结构及代码运行速度
~~并且尽全力督促队友爬取数据，虽然最后使用的还是只有我自己的数据。~~

### 4.1-4.8
> 根据已有数据，对数据进行初步筛选，严格保证筛选过程的客观性，将数据提取为Excel表格并对数据显示进行美化分发给队员进行人工识别虚假招聘
> 阅读了国外的一篇有关在线招聘虚假识别的论文，通过论文初步理解了判别信息真伪的原理，理解了大致的流程；

4.1-4.4：对数据库中的数据进行详细的分析，包括对每一个表字段的含义分析，重点分析了统计数据类的字段，还有各个字段之间的相关性分析、每个字段对虚假招聘判别的作用影响
4.5-4.8：确定了初步筛选的规则，并实现了数据筛选及展示的脚本，分发给队员进行判断

### 4.9-4.13
> 对部分数据进行人工判断，通过人工判断发现智联上面虚假招聘相对较少，不适合进一步的研究，故选择更换在线招聘的平台为58 同城
> 实现了 58 同城的招聘信息爬虫

4.9：58 同城招聘信息接口解析完成
4.10：58 同城招聘信息版本1.0 完成：以招聘信息为主的爬取方式
4.11：58 同城网页改版，之前完成的大部分代码重写，主要是公司详情页面
4.12：与师兄商讨数据库的建立问题，最终确定爬虫思路为：以公司信息为主的爬取方式，建立了最终的数据库
4.13：完成所有子模块，包括爬虫核心模块、数据库操作模块、主控制调度模块及配置模块，并完成了所有子模块的组装，58 同城招聘信息 2.0 完成

### 4.13-4.19

> 爬取了 58 同城两万余条招聘信息

其间多次修正代码逻辑、bug，多次优化数据库结构、增加部分有用的字段】去除部分无用字段，~~再次尽全力督促队友爬取数据。~~
经过对数据库中信息的大致观察，发现58 同城招聘的虚假情况较为严重，适合作为本次项目的研究数据

### 4.20-4.22

> 确定人工识别数据思路并进行分工，将数据划分、分发交由人工识别 

沿用之前的划分脚本（脚本包括数据库查询建表脚本、数据导出及数据划分脚本）。但是进一步优化了代码，在运行脚本之前先对现有数据进行一定的处理包括去除冗余数据等等。

### 4.23-4.26

> 对部分数据进行了识别之后发现对于虚假招聘的定义过于模糊，部分数据属性没有参考价值，以及缺乏统一数据判别规则，故开始着手完善上述问题

关于虚假招聘的定义：我们认为之前参考论文里提到的定义有一些狭隘，故决定在此基础上对其完善；

部分属性没有参考价值的问题早已发现，这次决定彻底去掉这些属性；

缺乏统一的数据判别规则问题，决定在参考论文实验统计结果的基础上结合我们对于招聘信息的经验以及国内招聘现状，制定一个尽量客观的招聘判别规则。

### 4.26-4.28

> 手工判别招聘信息规则 1.0 完成

下一步准备在判别规则的基础上，然后将规则里提到的异常点标号，形成一个手册。

### 4.28-4.30

> 异常点标号完成，手册完成，完成对虚假照聘的初步定义

### 5.1-5.4

> 人工数据判别完成一半，项目解决方案（初版）完成

### 5.5-5.7

> 人工数据判别完成，项目解决方案（终版）完成，开始进行数据汇总及分析

### 5.8-5.11

> 数据分析完成，特征工程架构完成

### 5.12-5.20

> 完成特征工程（初版）

### 5.21-5.26

> 完成分类器优化及其结果分析，选出最优模型，完成作品网站设计，demo设计，完成作品文档

### 5.27-5.28

> 彻底完成项目，提交项目

